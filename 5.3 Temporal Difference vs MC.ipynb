{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.3. \n",
    "\n",
    "![img](imgs/Screenshot_2018-05-21_20-37-36.png)\n",
    "![img](imgs/Screenshot_2018-05-21_20-38-07.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as IT\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "from set_up_random_walk import Random_Walk\n",
    "\n",
    "env = Random_Walk()\n",
    "\n",
    "#initial parameters\n",
    "gamma = env.gamma\n",
    "alpha = env.alpha\n",
    "R = env.R\n",
    "P = env.P\n",
    "\n",
    "N_states = env.N_states\n",
    "N_actions = env.N_actions\n",
    "terminal_states = env.terminal_states\n",
    "\n",
    "v_ini = env.v_ini\n",
    "q_ini = env.q_ini\n",
    "\n",
    "pi_rp = env.pi_rp\n",
    "pi_opt = env.pi_opt\n",
    "pi_off = env.pi_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# function to evaluate the state value(V) function of a certain policy\n",
    "def eval_v(policy):\n",
    "    P_pi = np.matmul(policy, P)\n",
    "    R_pi = np.matmul(policy, R)\n",
    "    return np.matmul(inv(np.identity(P_pi.shape[0]) - gamma * P_pi), R_pi)\n",
    "\n",
    "# function to evaluate the state-action(Q) value function of a certain policy\n",
    "def eval_q(policy):\n",
    "    # product of the transition matrix with the policy\n",
    "    P_aux = np.matmul(P, policy)\n",
    "    return np.matmul(inv(np.identity(P_aux.shape[0]) - gamma * P_aux), R)\n",
    "\n",
    "# rewards for a state\n",
    "def R_state(state):\n",
    "    return R[N_actions * state: N_actions * state + N_actions]\n",
    "\n",
    "# transition probabilities for a state\n",
    "def P_state(state):\n",
    "    return np.flip(P[N_actions * state: N_actions * (state+ 1) ,],0)\n",
    "    #return P[N_actions * state: N_actions * (state+ 1) ,]\n",
    "\n",
    "def R_(state, action=None):\n",
    "    if action == None:\n",
    "        return R_state(state)\n",
    "    else:\n",
    "        return R_state(state)[action]\n",
    "\n",
    "def P_(state, action=None, state_t1=None):\n",
    "    if action == None:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)\n",
    "        else:\n",
    "            return P_state(state)[:, state_t1]\n",
    "    else:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)[action,]\n",
    "        else:\n",
    "            return P_state(state)[action, state_t1]\n",
    "\n",
    "def policy_(policy, state, action=None): \n",
    "    # We have to treat them different if they are lists or numpy matrices\n",
    "    if type(policy)==list:\n",
    "        if action is not None:\n",
    "            return policy[state][state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state][state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "    else:\n",
    "        if action is not None:\n",
    "            return policy[state,state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state,state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "\n",
    "def generate_episode(policy, starting_pos=None):\n",
    "    episode = []\n",
    "    #if we dont have a starting position we choose randomly from the states\n",
    "    s_t0 = np.zeros(N_states)\n",
    "    if starting_pos==None:\n",
    "        random_start = np.random.randint(N_states)\n",
    "        s_t0[random_start] = 1\n",
    "    elif isinstance(starting_pos, list): # when given an array\n",
    "        #print('array')\n",
    "        s_t0 = starting_pos\n",
    "    elif isinstance(starting_pos, int):  # when given a number\n",
    "        #print('number')\n",
    "        random_start = starting_pos\n",
    "        s_t0[random_start] = 1\n",
    "    else:                                # assume is ndarray, lazy programming, sry\n",
    "        #print('ndarray')                 # it wont work tho\n",
    "        s_t0 = starting_pos\n",
    "     \n",
    "    s = np.random.choice(N_states, p=s_t0)\n",
    "    r = None\n",
    "    while s not in terminal_states:\n",
    "        a = np.random.choice(N_actions, size=1, p=policy_(policy,s))\n",
    "        r = R_(s,a)\n",
    "        episode.append({'s':s,'a':a[0],'r':np.array(r)[0,0]})\n",
    "        \n",
    "        #next state is chosen from the transition matrix from state s taken action a\n",
    "        #s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "        s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "    episode.append({'s':s,'a':0,'r':0})\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](imgs/4.3.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TD method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_episodes = 800\n",
    "\n",
    "def temporal_difference_v(policy):\n",
    "    v = np.zeros(N_states)#1\n",
    "    for _ in range(N_episodes):#2,10\n",
    "        random_start = np.random.randint(N_states)#3\n",
    "        s = random_start\n",
    "        while s not in terminal_states:#4,9\n",
    "            a = np.random.choice(N_actions, size=1, p=policy_(policy,s))#5\n",
    "            r = R_(s,a)#6\n",
    "            s_t1 = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())#6\n",
    "            v[s] = v[s] + alpha*(r+v[s_t1]-v[s])#7\n",
    "            s = s_t1#8\n",
    "    return v #11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The montecarlo method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_MC_policy_evaluation_v(policy,starting_pos=None):\n",
    "    i=0\n",
    "    v = np.zeros(N_states)#1\n",
    "    N = np.zeros(N_states)#2\n",
    "    delta = float('inf')\n",
    "    epsilon = 0.0001\n",
    "    g=0\n",
    "    while delta > epsilon:#3,13\n",
    "        delta = 0 #4\n",
    "        \"\"\"Generate an episode using pi: {s0,a0,r1,....,sT-1,aT-2,rT} \n",
    "        Each episode is a list of dictionaries containing 's_t','a_t','r_t+1'\n",
    "        \n",
    "        episode = [{'s_t':s,'a_t':a,'r_t+1':r},....]\n",
    "        T is the length of the episode\n",
    "        \"\"\"\n",
    "        episode = generate_episode(policy,starting_pos)#5\n",
    "        i+=1\n",
    "        T = len(episode)\n",
    "        #print('episode ', i,' length T: ',T)\n",
    "        g = 0 #6\n",
    "        for t in reversed(range(T)): #7\n",
    "            \n",
    "            s = episode[t]['s']\n",
    "            r = episode[t]['r']\n",
    "            g = gamma*g + r #8\n",
    "            #print( '   s?',s,'a?',episode[t]['a'],'r?',r,'g?', g)\n",
    "            N[s] += 1 #9\n",
    "            v_old = v[s] #10\n",
    "            v[s] = v[s] + (1/N[s])*(g - v[s]) #11\n",
    "            delta = max(delta, np.abs(v_old-v[s])) #12\n",
    "            \n",
    "        #print(delta)\n",
    "    return v  #14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving the with TD methods:\n",
      "\n",
      "[0.   0.12 0.26 0.41 0.59 0.8  0.  ]\n",
      "\n",
      "Monte-Carlo Policy evaluation every visit:\n",
      "\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "v = temporal_difference_v(pi_rp)\n",
    "print('\\nSolving the with TD methods:\\n')\n",
    "print(v)\n",
    "#If the V function is 0 repeat because it has not found any reward in the first episode and the delta still is 0\n",
    "v_MC = every_visit_MC_policy_evaluation_v(pi_rp)\n",
    "print('\\nMonte-Carlo Policy evaluation every visit:\\n')\n",
    "print(v_MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again the unreliability of MC methods with this implementation makes it so you have to repeat some times before working*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
