{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.2.\n",
    "Implement PI in Matlab and compute the optimal policy and\n",
    "optimal value function for the problem described in Exercise 3.3. Compare\n",
    "the results obtained through PI with the theoretical result obtained in Exercise 3.3.\n",
    "![img](imgs/Screenshot_2018-04-20_11-23-41.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "#initial parameters\n",
    "gamma = .9\n",
    "R = np.matrix([[-1],[.6],[.5],[-.9]])\n",
    "P = np.matrix([[0.8,0.2],[0.2,0.8],[0.3,0.7],[0.9,0.1]])\n",
    "\n",
    "N_states = 2\n",
    "N_actions = 2\n",
    "N_steps = 400\n",
    "N_steps_pe = 20 \n",
    "N_steps_pi = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "import itertools as IT\n",
    "\n",
    "\n",
    "# function to evaluate the state value(V) function of a certain policy\n",
    "def eval_v(policy):\n",
    "    P_pi = np.matmul(policy, P)\n",
    "    R_pi = np.matmul(policy, R)\n",
    "\n",
    "    return np.matmul(inv(np.identity(P_pi.shape[0]) - gamma * P_pi), R_pi)\n",
    "\n",
    "\n",
    "# function to evaluate the state-action(Q) value function of a certain policy\n",
    "def eval_q(policy):\n",
    "    # product of the transition matrix with the policy\n",
    "    P_aux = np.matmul(P, policy)\n",
    "\n",
    "    return np.matmul(inv(np.identity(P_aux.shape[0]) - gamma * P_aux), R)\n",
    "\n",
    "\n",
    "# rewards for a state\n",
    "def R_state(state):\n",
    "    return R[N_actions * state: N_actions * state + N_actions]\n",
    "\n",
    "\n",
    "# transition probabilities for a state\n",
    "def P_state(state):\n",
    "    return P[N_actions * state: N_actions * state + N_actions, ]\n",
    "\n",
    "\n",
    "def R_(state, action=None):\n",
    "    if action == None:\n",
    "        return R_state(state)\n",
    "    else:\n",
    "        return R_state(state)[action]\n",
    "\n",
    "\n",
    "def P_(state, action=None, state_t1=None):\n",
    "    if action == None:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)\n",
    "        else:\n",
    "            return P_state(state)[:, state_t1]\n",
    "    else:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)[action,]\n",
    "        else:\n",
    "            return P_state(state)[action, state_t1]\n",
    "\n",
    "\n",
    "def policy_(policy, state, action=None):\n",
    "    if action is not None:\n",
    "        return policy[state][state * N_actions + action]\n",
    "    else:\n",
    "        return [policy[state][state * N_actions + action_] for action_ in range(N_actions)]\n",
    "        \n",
    "\n",
    "# We define the algorithm for policy evaluation with the state value function\n",
    "def policy_evaluation_v(policy):\n",
    "    # 1\n",
    "    v = np.zeros(N_states)\n",
    "    # 2, 8 stop condition is not stated, instead we do 400 iterations\n",
    "    for i in range(N_steps):\n",
    "        # 3\n",
    "        delta = 0\n",
    "        # 4\n",
    "        for state in range(N_states):\n",
    "            # 5\n",
    "            v_old = v[state]\n",
    "            # 6\n",
    "            v_aux = 0\n",
    "            for action in range(N_actions):\n",
    "                v_aux += policy_(policy, state, action) * (R_(state, action) + gamma * sum(\n",
    "                    [P_(state, action, state_t1) * v[state_t1] for state_t1 in range(N_states)]))\n",
    "            v[state] = v_aux\n",
    "            # 7\n",
    "            # delta = np.max(delta, np.abs(v_old - v[state]))\n",
    "\n",
    "    # 8,9\n",
    "    return v\n",
    "\n",
    "\n",
    "# We define the algorithm for policy evaluation with the state-action\n",
    "# value function\n",
    "# We define the algorithm for policy evaluation with the state value function\n",
    "def policy_evaluation_q(policy):\n",
    "    # 1\n",
    "    q = np.zeros(N_states * N_actions)\n",
    "    # 2, 8 stop condition is not stated, instead we do 400 iterations\n",
    "    for i in range(N_steps):\n",
    "        # 3\n",
    "        delta = 0\n",
    "        # 4 TO-DO\n",
    "        for state, action in IT.product(range(N_states), range(N_actions)):\n",
    "            # 5\n",
    "            q_old = q[state * N_actions + action]\n",
    "            # 6\n",
    "            q_aux = [R_(state, action)]\n",
    "            for state_t1 in range(N_states):\n",
    "                q_aux += gamma * P_(state, action, state_t1) * sum(\n",
    "                    [policy_(policy, state_t1, action_t1) * q[state_t1 * N_actions + action_t1] for action_t1 in\n",
    "                     range(N_actions)])\n",
    "            q[state * N_actions + action] = q_aux\n",
    "            # 7\n",
    "            # delta = np.max(delta, np.abs(q_old - q[state,action]))\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration framework\n",
    "![img](imgs/Screenshot_2018-05-07_14-07-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Policy Iteration for V functions\n",
    "![img](imgs/Screenshot_2018-05-07_14-07-51.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy iteration for state value function\n",
    "def policy_iteration_v(policy, debug=False):\n",
    "    v = np.zeros(N_states) #1\n",
    "    theta = False\n",
    "    while not theta: #2,3\n",
    "        v = policy_evaluation_v(policy) # 4-9\n",
    "        if debug : print('\\n v:',v)\n",
    "        theta = True #10\n",
    "        if debug : print('For each state: ')\n",
    "        for s in range(N_states): #11\n",
    "            if debug : print('\\n s:',s)\n",
    "            a = policy_(policy, s) #12\n",
    "            if debug : print('a:',a)  \n",
    "            \n",
    "            #13\n",
    "            arg_max = np.argmax([(R_(s, a_t1) + gamma *\n",
    "                                 np.sum([np.dot(P_(s, a_t1, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                                 for a_t1 in range(N_actions)])\n",
    "            if debug : print('arg_max:',arg_max)\n",
    "            policy[s] = [0] * len(policy[s])\n",
    "            policy[s][N_actions * s + arg_max] = 1\n",
    "            \n",
    "            if debug : print('pi[s]:',policy[s])\n",
    "            if debug : print('policy_(policy, s):',policy_(policy, s))\n",
    "            if not (a == policy_(policy, s)): \n",
    "                theta = False #14\n",
    "                if debug : print('a not equal to policy_(policy, s)')\n",
    "                \n",
    "            if debug : print('theta:',theta)\n",
    "        if debug : print('\\n pi:',policy)\n",
    "    return policy #15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-  I leave the prints commented in case of the need of debugging*\n",
    "\n",
    "*-  The creation of this method was pretty intrincate, if I wanted to do the expression of line #13 in one line I would have to have nested a lot of commands. The way it is made is a compromise between readability and space( which also affects readability after all)*\n",
    "\n",
    "*- To show that they work, i will print their inner workings step by step below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_random: \n",
      " [[0.68 0.32 0.   0.  ]\n",
      " [0.   0.   0.56 0.44]]\n",
      "\n",
      " policy iteration with v: \n",
      " [[0 1 0 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#we generate 2 probabilities of taking each action from each state\n",
    "p = uniform(0, 1)\n",
    "q = uniform(0, 1)\n",
    "# We generate the policy from those 2 probabilities, making sure no invalid actions are taken\n",
    "pi_random = [[p,1 - p,0,0],[0,0,q,1 - q]]\n",
    "print('pi_random: \\n',np.array(pi_random))\n",
    "policy_iteration_v(pi_random)\n",
    "print('\\n policy iteration with v: \\n',np.array(policy_iteration_v(pi_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration for Q functions\n",
    "![img](imgs/Screenshot_2018-05-07_14-08-03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy iteration for state-action value function\n",
    "def policy_iteration_q(policy, debug=False):\n",
    "    q = np.zeros(N_states * N_actions) #1\n",
    "    theta = False\n",
    "    while not theta: #2,3\n",
    "        q = policy_evaluation_q(policy) # 4-9\n",
    "        if debug : print('\\n q:',q)\n",
    "        theta = True #10\n",
    "        # print('\\n For each state: ')\n",
    "        for s in range(N_states): #11\n",
    "            if debug : print('\\n s:',s)\n",
    "            a = policy_(policy, s) #12\n",
    "            if debug : print('a:',a)  \n",
    "            \n",
    "            #13\n",
    "            arg_max = np.argmax([q[s * N_actions + a_t1]  for a_t1 in range(N_actions)])\n",
    "            if debug : print('arg_max:',arg_max)\n",
    "            policy[s] = [0] * len(policy[s])\n",
    "            policy[s][N_actions * s + arg_max] = 1\n",
    "            \n",
    "            if debug : print('pi[s]:',policy[s])\n",
    "            if debug : print('policy_(policy, s):',policy_(policy, s))\n",
    "            if not (a == policy_(policy, s)): \n",
    "                theta = False #14\n",
    "                if debug : print('a not equal to policy_(policy, s)')\n",
    "                \n",
    "            if debug : print('theta:',theta)\n",
    "        if debug : print('pi:',policy)\n",
    "    return policy #15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*- This was way easier to implement due to the fact that we dont have to loop over actions and states, but only over actions. This is because the q already contains this information.*\n",
    "\n",
    "*- To show that they work, i will print their inner workings step by step below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_random: \n",
      " [[0.19 0.81 0.   0.  ]\n",
      " [0.   0.   0.28 0.72]]\n",
      "\n",
      " policy iteration with v: \n",
      " [[0 1 0 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#we generate 2 probabilities of taking each action from each state\n",
    "p = uniform(0, 1)\n",
    "q = uniform(0, 1)\n",
    "# We generate the policy from those 2 probabilities, making sure no invalid actions are taken\n",
    "pi_random = [[p,1 - p,0,0],[0,0,q,1 - q]]\n",
    "print('pi_random: \\n',np.array(pi_random))\n",
    "policy_iteration_q(pi_random)\n",
    "print('\\n policy iteration with v: \\n',np.array(policy_iteration_v(pi_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step policy iteration with V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we generate 2 probabilities of taking each action from each state\n",
    "p = uniform(0, 1)\n",
    "q = uniform(0, 1)\n",
    "# We generate the policy from those 2 probabilities, making sure no invalid actions are taken\n",
    "pi_random = [[p,1 - p,0,0],[0,0,q,1 - q]]\n",
    "print('pi_random: \\n',np.array(pi_random))\n",
    "\n",
    "print('\\n policy iteration with v: \\n',np.array(policy_iteration_v(pi_random,True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step policy iteration with Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we generate 2 probabilities of taking each action from each state\n",
    "p = uniform(0, 1)\n",
    "q = uniform(0, 1)\n",
    "# We generate the policy from those 2 probabilities, making sure no invalid actions are taken\n",
    "pi_random = [[p,1 - p,0,0],[0,0,q,1 - q]]\n",
    "print('pi_random: \\n',np.array(pi_random))\n",
    "\n",
    "print('\\n policy iteration with v: \\n',np.array(policy_iteration_v(pi_random,True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
