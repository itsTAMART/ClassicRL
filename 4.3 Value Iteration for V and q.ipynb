{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.3. \n",
    "Implement VI in Matlab and compute the optimal policy\n",
    "and optimal value function for the problem described in Exercise 3.3. Com-\n",
    "pare the results obtained through VI with the theoretical result obtained in\n",
    "Exercise3.3 Compare results obtaiued by VI and PI.\n",
    "![img](imgs/Screenshot_2018-04-20_11-23-41.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as IT\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "#initial parameters\n",
    "gamma = .9\n",
    "R = np.matrix([[-1],[.6],[.5],[-.9]])\n",
    "P = np.matrix([[0.8,0.2],[0.2,0.8],[0.3,0.7],[0.9,0.1]])\n",
    "\n",
    "N_states = 2\n",
    "N_actions = 2\n",
    "N_steps = 400\n",
    "N_steps_pe = 20 \n",
    "N_steps_pi = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "import itertools as IT\n",
    "\n",
    "\n",
    "# function to evaluate the state value(V) function of a certain policy\n",
    "def eval_v(policy):\n",
    "    P_pi = np.matmul(policy, P)\n",
    "    R_pi = np.matmul(policy, R)\n",
    "\n",
    "    return np.matmul(inv(np.identity(P_pi.shape[0]) - gamma * P_pi), R_pi)\n",
    "\n",
    "\n",
    "# function to evaluate the state-action(Q) value function of a certain policy\n",
    "def eval_q(policy):\n",
    "    # product of the transition matrix with the policy\n",
    "    P_aux = np.matmul(P, policy)\n",
    "\n",
    "    return np.matmul(inv(np.identity(P_aux.shape[0]) - gamma * P_aux), R)\n",
    "\n",
    "\n",
    "# rewards for a state\n",
    "def R_state(state):\n",
    "    return R[N_actions * state: N_actions * state + N_actions]\n",
    "\n",
    "\n",
    "# transition probabilities for a state\n",
    "def P_state(state):\n",
    "    return P[N_actions * state: N_actions * state + N_actions, ]\n",
    "\n",
    "\n",
    "def R_(state, action=None):\n",
    "    if action == None:\n",
    "        return R_state(state)\n",
    "    else:\n",
    "        return R_state(state)[action]\n",
    "\n",
    "\n",
    "def P_(state, action=None, state_t1=None):\n",
    "    if action == None:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)\n",
    "        else:\n",
    "            return P_state(state)[:, state_t1]\n",
    "    else:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)[action,]\n",
    "        else:\n",
    "            return P_state(state)[action, state_t1]\n",
    "\n",
    "\n",
    "def policy_(policy, state, action=None):\n",
    "    if action is not None:\n",
    "        return policy[state][state * N_actions + action]\n",
    "    else:\n",
    "        return [policy[state][state * N_actions + action_] for action_ in range(N_actions)]\n",
    "        \n",
    "\n",
    "# We define the algorithm for policy evaluation with the state value function\n",
    "def policy_evaluation_v(policy):\n",
    "    # 1\n",
    "    v = np.zeros(N_states)\n",
    "    # 2, 8 stop condition is not stated, instead we do 400 iterations\n",
    "    for i in range(N_steps):\n",
    "        # 3\n",
    "        delta = 0\n",
    "        # 4\n",
    "        for state in range(N_states):\n",
    "            # 5\n",
    "            v_old = v[state]\n",
    "            # 6\n",
    "            v_aux = 0\n",
    "            for action in range(N_actions):\n",
    "                v_aux += policy_(policy, state, action) * (R_(state, action) + gamma * sum(\n",
    "                    [P_(state, action, state_t1) * v[state_t1] for state_t1 in range(N_states)]))\n",
    "            v[state] = v_aux\n",
    "            # 7\n",
    "            # delta = np.max(delta, np.abs(v_old - v[state]))\n",
    "\n",
    "    # 8,9\n",
    "    return v\n",
    "\n",
    "\n",
    "# We define the algorithm for policy evaluation with the state-action\n",
    "# value function\n",
    "# We define the algorithm for policy evaluation with the state value function\n",
    "def policy_evaluation_q(policy):\n",
    "    # 1\n",
    "    q = np.zeros(N_states * N_actions)\n",
    "    # 2, 8 stop condition is not stated, instead we do 400 iterations\n",
    "    for i in range(N_steps):\n",
    "        # 3\n",
    "        delta = 0\n",
    "        # 4 TO-DO\n",
    "        for state, action in IT.product(range(N_states), range(N_actions)):\n",
    "            # 5\n",
    "            q_old = q[state * N_actions + action]\n",
    "            # 6\n",
    "            q_aux = [R_(state, action)]\n",
    "            for state_t1 in range(N_states):\n",
    "                q_aux += gamma * P_(state, action, state_t1) * sum(\n",
    "                    [policy_(policy, state_t1, action_t1) * q[state_t1 * N_actions + action_t1] for action_t1 in\n",
    "                     range(N_actions)])\n",
    "            q[state * N_actions + action] = q_aux\n",
    "            # 7\n",
    "            # delta = np.max(delta, np.abs(q_old - q[state,action]))\n",
    "\n",
    "    return q\n",
    "\n",
    "# Policy iteration for state value function\n",
    "def policy_iteration_v(policy):\n",
    "    v = np.zeros(N_states) #1\n",
    "    theta = False\n",
    "    while not theta: #2,3\n",
    "        v = policy_evaluation_v(policy) # 4-9\n",
    "        #print('\\n v:',v)\n",
    "        theta = True #10\n",
    "        for s in range(N_states): #11\n",
    "            #print('\\n s:',s)\n",
    "            a = policy_(policy, s) #12\n",
    "            #print('a:',a)  \n",
    "            \n",
    "            #13\n",
    "            arg_max = np.argmax([(R_(s, a_t1) + gamma *\n",
    "                                 np.sum([np.dot(P_(s, a_t1, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                                 for a_t1 in range(N_actions)])\n",
    "            #print('arg_max:',arg_max)\n",
    "            policy[s] = [0] * len(policy[s])\n",
    "            policy[s][N_actions * s + arg_max] = 1\n",
    "            \n",
    "            #print('pi[s]:',policy[s])\n",
    "            #print('policy_(policy, s):',policy_(policy, s))\n",
    "            if not (a == policy_(policy, s)): theta = False #14\n",
    "                #print('a not equal to policy_(policy, s)')\n",
    "                \n",
    "            #print('theta:',theta)\n",
    "        #print('pi:',policy)\n",
    "    return policy #15\n",
    "\n",
    "# Policy iteration for state-action value function\n",
    "def policy_iteration_q(policy):\n",
    "    q = np.zeros(N_states * N_actions) #1\n",
    "    theta = False\n",
    "    while not theta: #2,3\n",
    "        q = policy_evaluation_q(policy) # 4-9\n",
    "        #print('\\n q:',q)\n",
    "        theta = True #10\n",
    "        for s in range(N_states): #11\n",
    "            #print('\\n s:',s)\n",
    "            a = policy_(policy, s) #12\n",
    "            #print('a:',a)  \n",
    "            \n",
    "            #13\n",
    "            arg_max = np.argmax([q[s * N_actions + a_t1]  for a_t1 in range(N_actions)])\n",
    "            #print('arg_max:',arg_max)\n",
    "            policy[s] = [0] * len(policy[s])\n",
    "            policy[s][N_actions * s + arg_max] = 1\n",
    "            \n",
    "            #print('pi[s]:',policy[s])\n",
    "            #print('policy_(policy, s):',policy_(policy, s))\n",
    "            if not (a == policy_(policy, s)): theta = False #14\n",
    "                #print('a not equal to policy_(policy, s)')\n",
    "                \n",
    "            #print('theta:',theta)\n",
    "        #print('pi:',policy)\n",
    "    return policy #15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Value Iteration for V functions\n",
    "![img](imgs/4.3.1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_v(debug=False):\n",
    "    if debug : print('Value Iteration with V:')\n",
    "    policy = np.zeros([N_states,(N_states*N_actions)]) \n",
    "    v = np.zeros(N_states) #1\n",
    "    epsilon = 0.01\n",
    "    delta = 10000000\n",
    "    while delta>epsilon: #2\n",
    "        delta = 0 #3\n",
    "        \n",
    "        if debug : print('\\n v:',v)\n",
    "        for s in range(N_states): #4\n",
    "            if debug : print('\\n s:',s)\n",
    "            \n",
    "            v_old = v[s] #5\n",
    "            if debug : print('\\n v_old:',v_old)\n",
    "            #6\n",
    "            v[s]=np.max([(R_(s, a) + gamma *\n",
    "                         np.sum([np.dot(P_(s, a, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                         for a in range(N_actions)])\n",
    "            if debug : print('\\n v[s]:',v[s])\n",
    "            delta = max(delta, np.abs(v_old-v[s]))#8\n",
    "            if debug : print('delta:',delta)\n",
    "\n",
    "    # (out of the while) \n",
    "    if debug : print('\\n For each state: \\n')\n",
    "    for s in range(N_states): #9\n",
    "        #10\n",
    "        arg_max = np.argmax([(R_(s, a) + gamma *\n",
    "                             np.sum([np.dot(P_(s, a, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                             for a in range(N_actions)])\n",
    "        if debug : print('arg_max:',arg_max)\n",
    "        policy[s] = [0] * (N_states*N_actions)\n",
    "        policy[s][N_actions * s + arg_max] = 1\n",
    "            \n",
    "        if debug : print('pi[s]:',policy[s])\n",
    "\n",
    "\n",
    "    if debug : print('pi:',policy)\n",
    "    return policy #11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*- To prove that they work a will show a print step by step a few cells below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " value iteration with v: \n",
      " [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n value iteration with v: \\n',np.array(value_iteration_v()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration for Q functions\n",
    "![img](imgs/4.3.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_q(debug=False):\n",
    "    if debug : print('Value Iteration with Q:')\n",
    "    policy = np.zeros([N_states,(N_states*N_actions)]) \n",
    "    q = np.zeros(N_states*N_actions) #1\n",
    "    epsilon = 0.01\n",
    "    delta = 10000000\n",
    "    while delta>epsilon: # 2, 8\n",
    "        delta = 0 #3\n",
    "        \n",
    "        if debug : print('\\n q:',q)\n",
    "        if debug : print('\\n For each state action pair: \\n:')    \n",
    "        for s , a in IT.product(range(N_states),range(N_actions)): #4\n",
    "            if debug : print('\\n s , a:', s , a)\n",
    "            \n",
    "            q_old = q[s * N_actions + a] #5\n",
    "            if debug : print('\\n q_old:',q_old)\n",
    "            \n",
    "            #6\n",
    "            q[s * N_actions + a] = R_(s, a) + gamma * np.sum([np.dot(P_(s, a, s_t1), \n",
    "                                                    np.max([q[s_t1 * N_actions + a_t1] for a_t1 in range(N_actions)])) \n",
    "                                              for s_t1 in range(N_states)])\n",
    "           \n",
    "         \n",
    "            if debug : print('\\n q(s,a)',q[s * N_actions + a])\n",
    "            delta = max(delta, np.abs(q_old-q[s * N_actions + a]))#7\n",
    "            if debug : print('delta:',delta)\n",
    "\n",
    "    # (out of the while) \n",
    "    if debug : print('\\n For each state: \\n')\n",
    "    for s in range(N_states): #9\n",
    "        #10\n",
    "        arg_max = np.argmax([q[s * N_actions + a_t1]  for a_t1 in range(N_actions)])\n",
    "        if debug : print('arg_max:',arg_max)\n",
    "        policy[s] = [0] * (N_states*N_actions)\n",
    "        policy[s][N_actions * s + arg_max] = 1\n",
    "            \n",
    "        if debug : print('pi[s]:',policy[s])\n",
    "\n",
    "    if debug : print('pi:',policy)\n",
    "    return policy #11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*- To prove that they work a will show a print step by step a few cells below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " value iteration with q: \n",
      " [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n value iteration with q: \\n',np.array(value_iteration_q()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step by Step Value Iteration with V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n value iteration with q: \\n',np.array(value_iteration_v(True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step Value Iteration with Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n value iteration with q: \\n',np.array(value_iteration_q(True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
