{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.5. \n",
    "\n",
    "![img](imgs/5.5.1.png)\n",
    "![img](imgs/5.5.2.png)\n",
    "![img](imgs/5.5.3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as IT\n",
    "np.set_printoptions(precision=1, suppress=True)\n",
    "from set_up_cliff import Cliff\n",
    "\n",
    "env = Cliff()\n",
    "\n",
    "#initial parameters\n",
    "gamma = env.gamma\n",
    "alpha = env.alpha\n",
    "R = env.R\n",
    "P = env.P\n",
    "\n",
    "N_states = env.N_states\n",
    "N_actions = env.N_actions\n",
    "terminal_states = env.terminal_states\n",
    "\n",
    "v_ini = env.v_ini\n",
    "q_ini = env.q_ini\n",
    "\n",
    "pi_rp = env.pi_rp\n",
    "pi_opt = env.pi_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# function to evaluate the state value(V) function of a certain policy\n",
    "def eval_v(policy):\n",
    "    P_pi = np.matmul(policy, P)\n",
    "    R_pi = np.matmul(policy, R)\n",
    "    return np.matmul(inv(np.identity(P_pi.shape[0]) - gamma * P_pi), R_pi)\n",
    "\n",
    "# function to evaluate the state-action(Q) value function of a certain policy\n",
    "def eval_q(policy):\n",
    "    # product of the transition matrix with the policy\n",
    "    P_aux = np.matmul(P, policy)\n",
    "    return np.matmul(inv(np.identity(P_aux.shape[0]) - gamma * P_aux), R)\n",
    "\n",
    "# rewards for a state\n",
    "def R_state(state):\n",
    "    return R[N_actions * state: N_actions * state + N_actions]\n",
    "\n",
    "# transition probabilities for a state\n",
    "def P_state(state):\n",
    "    return np.flip(P[N_actions * state: N_actions * (state+ 1) ,],0)\n",
    "    #return P[N_actions * state: N_actions * (state+ 1) ,]\n",
    "\n",
    "def R_(state, action=None):\n",
    "    if action == None:\n",
    "        return R_state(state)\n",
    "    else:\n",
    "        return R_state(state)[action]\n",
    "\n",
    "def P_(state, action=None, state_t1=None):\n",
    "    if action == None:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)\n",
    "        else:\n",
    "            return P_state(state)[:, state_t1]\n",
    "    else:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)[action,]\n",
    "        else:\n",
    "            return P_state(state)[action, state_t1]\n",
    "\n",
    "def policy_(policy, state, action=None): \n",
    "    # We have to treat them different if they are lists or numpy matrices\n",
    "    if type(policy)==list:\n",
    "        if action is not None:\n",
    "            return policy[state][state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state][state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "    else:\n",
    "        if action is not None:\n",
    "            return policy[state,state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state,state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "\n",
    "def generate_episode(policy, starting_pos=None):\n",
    "    episode = []\n",
    "    #if we dont have a starting position we choose randomly from the states\n",
    "    s_t0 = np.zeros(N_states)\n",
    "    if starting_pos==None:\n",
    "        random_start = np.random.randint(N_states)\n",
    "        s_t0[random_start] = 1\n",
    "    elif isinstance(starting_pos, list): # when given an array\n",
    "        #print('array')\n",
    "        s_t0 = starting_pos\n",
    "    elif isinstance(starting_pos, int):  # when given a number\n",
    "        #print('number')\n",
    "        random_start = starting_pos\n",
    "        s_t0[random_start] = 1\n",
    "    else:                                # assume is ndarray, lazy programming, sry\n",
    "        #print('ndarray')                 # it wont work tho\n",
    "        s_t0 = starting_pos\n",
    "     \n",
    "    s = np.random.choice(N_states, p=s_t0)\n",
    "    r = None\n",
    "    while s not in terminal_states:\n",
    "        a = np.random.choice(N_actions, size=1, p=policy_(policy,s))\n",
    "        r = R_(s,a)\n",
    "        episode.append({'s':s,'a':a[0],'r':np.array(r)[0,0]})\n",
    "        \n",
    "        #next state is chosen from the transition matrix from state s taken action a\n",
    "        #s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "        s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "    episode.append({'s':s,'a':0,'r':0})\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimum Policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '⬇️']\n",
      " ['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '⬇️']\n",
      " ['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '⬇️']\n",
      " ['⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬇️']]\n",
      "V with policy evaluation \n",
      "\n",
      "[[ -3.7  -2.9  -2.9  -2.9  -2.9  -2.9  -2.9  -2.9  -2.9  -2.9  -2.9 -20. ]\n",
      " [ -2.9  -1.9  -1.9  -1.9  -1.9  -1.9  -1.9  -1.9  -1.9  -1.9  -1.9 -20. ]\n",
      " [ -1.9  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  -20. ]\n",
      " [ -1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   -1.9]]\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation_v(policy):\n",
    "    # 1\n",
    "    v = np.zeros(N_states)\n",
    "    epsilon = 0.001\n",
    "    delta = float('inf')\n",
    "    # 2, 8 stop condition is not stated, instead we do 400 iterations\n",
    "    while delta>epsilon:\n",
    "    #for i in range(N_steps):\n",
    "        # 3\n",
    "        delta = 0\n",
    "        # 4\n",
    "        for state in range(N_states):\n",
    "            # 5\n",
    "            v_old = v[state]\n",
    "            # 6\n",
    "            v_aux = 0\n",
    "            for action in range(N_actions):\n",
    "                v_aux += policy_(policy, state, action) * (R_(state, action) + gamma * sum(\n",
    "                    [P_(state, action, state_t1) * v[state_t1] for state_t1 in range(N_states)]))\n",
    "            v[state] = v_aux\n",
    "            # 7\n",
    "            delta = max(delta, np.abs(v_old - v[state]))\n",
    "\n",
    "    # 8,9\n",
    "    return v\n",
    "\n",
    "env.plot_policy(pi_opt)\n",
    "print('V with policy evaluation \\n')\n",
    "print(np.reshape(policy_evaluation_v(pi_opt), (4, 12))[::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VI and PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALUE ITERATION \n",
      "\n",
      "[['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '⬆️']\n",
      " ['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '⬆️']\n",
      " ['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '⬆️']\n",
      " ['⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️']]\n",
      "\n",
      " V with policy evaluation \n",
      "\n",
      "[[-3.7 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -3.7]\n",
      " [-2.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -2.9]\n",
      " [-1.9 -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.9]\n",
      " [-1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.9]]\n",
      "\n",
      "POLICY ITERATION \n",
      "\n",
      "[['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '⬆️']\n",
      " ['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '⬆️']\n",
      " ['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '⬆️']\n",
      " ['⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️']]\n",
      "\n",
      " V with policy evaluation \n",
      "\n",
      "[[-3.7 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -3.7]\n",
      " [-2.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -2.9]\n",
      " [-1.9 -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.9]\n",
      " [-1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.9]]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration_v():\n",
    "    policy = np.zeros([N_states, (N_states * N_actions)])\n",
    "    v = np.zeros(N_states)  # 1\n",
    "    epsilon = 0.01\n",
    "    delta = float('inf')\n",
    "    while delta > epsilon:  # 2\n",
    "        delta = 0  # 3\n",
    "\n",
    "        # print('\\n v:',v)\n",
    "        for s in range(N_states):  # 4\n",
    "            # print('\\n s:',s)\n",
    "\n",
    "            v_old = v[s]  # 5\n",
    "            # print('\\n v_old:',v_old)\n",
    "            # 6\n",
    "            v[s] = max([(R_(s, a) + gamma *\n",
    "                            np.sum([np.dot(P_(s, a, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                           for a in range(N_actions)])\n",
    "            # print('\\n v[s]:',v[s])\n",
    "            delta = max(delta, np.abs(v_old - v[s]))  # 8\n",
    "            # print('delta:',delta)\n",
    "\n",
    "    # (out of the while)\n",
    "    for s in range(N_states):  # 9\n",
    "        # 10\n",
    "        arg_max = np.argmax([(R_(s, a) + gamma *\n",
    "                              np.sum([np.dot(P_(s, a, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                             for a in range(N_actions)])\n",
    "        # print('arg_max:',arg_max)\n",
    "        policy[s] = [0] * (N_states * N_actions)\n",
    "        policy[s][N_actions * s + arg_max] = 1\n",
    "\n",
    "        # print('pi[s]:',policy[s])\n",
    "\n",
    "    # print('pi:',policy)\n",
    "    return policy  # 11\n",
    "\n",
    "\n",
    "# Policy iteration for state value function\n",
    "def policy_iteration_v(policy):\n",
    "    v = np.zeros(N_states)  # 1\n",
    "    theta = False\n",
    "    while not theta:  # 2,3\n",
    "        v = policy_evaluation_v(policy)  # 4-9\n",
    "        # print('\\n v:',v)\n",
    "        theta = True  # 10\n",
    "        for s in range(N_states):  # 11\n",
    "            # print('\\n s:',s)\n",
    "            a = policy_(policy, s)  # 12\n",
    "            # print('a:',a)\n",
    "\n",
    "            # 13\n",
    "            arg_max = np.argmax([(R_(s, a_t1) + gamma *\n",
    "                                  np.sum([np.dot(P_(s, a_t1, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                                 for a_t1 in range(N_actions)])\n",
    "            # print('arg_max:',arg_max)\n",
    "            policy[s] = [0] * len(policy[s])\n",
    "            policy[s][N_actions * s + arg_max] = 1\n",
    "\n",
    "            # print('pi[s]:',policy[s])\n",
    "            # print('policy_(policy, s):',policy_(policy, s))\n",
    "            if not (a == policy_(policy, s)): theta = False  # 14\n",
    "            # print('a not equal to policy_(policy, s)')\n",
    "\n",
    "            # print('theta:',theta)\n",
    "        # print('pi:',policy)\n",
    "    return policy  # 15\n",
    "\n",
    "print('\\nVALUE ITERATION \\n')\n",
    "pi_vi = value_iteration_v()\n",
    "env.plot_policy(pi_vi)\n",
    "print('\\n V with policy evaluation \\n')\n",
    "print(np.reshape(policy_evaluation_v(pi_vi), (4, 12))[::-1])\n",
    "\n",
    "print('\\nPOLICY ITERATION \\n')\n",
    "pi_pi = policy_iteration_v(np.copy(pi_rp))\n",
    "env.plot_policy(pi_pi)\n",
    "print('\\n V with policy evaluation \\n')\n",
    "print(np.reshape(policy_evaluation_v(pi_pi), (4, 12))[::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-Learning and SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-LEARNING \n",
      "\n",
      "[['⬇️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️']\n",
      " ['⬇️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️']\n",
      " ['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️']\n",
      " ['⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️']]\n",
      "\n",
      " V with policy evaluation \n",
      "\n",
      "[[-3.7 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -2.9 -4.4]\n",
      " [-2.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -1.9 -3.6]\n",
      " [-1.9 -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -2.8]\n",
      " [-1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.9]]\n",
      "\n",
      "SARSA \n",
      "\n",
      "[['➡️' '⬅️' '➡️' '➡️' '➡️' '⬆️' '➡️' '➡️' '➡️' '⬆️' '➡️' '➡️']\n",
      " ['➡️' '⬇️' '⬇️' '➡️' '⬆️' '➡️' '⬆️' '⬇️' '➡️' '⬆️' '⬇️' '➡️']\n",
      " ['➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️' '➡️']\n",
      " ['⬇️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️' '⬆️']]\n",
      "\n",
      " V with policy evaluation \n",
      "\n",
      "[[ -3.7 -20.   -3.7  -2.9  -3.7  -4.5  -3.7  -3.7  -2.9  -3.7  -5.2  -4.4]\n",
      " [ -2.9  -3.7  -2.9  -1.9  -2.9  -1.9  -2.9  -2.9  -1.9  -2.9  -4.4  -3.6]\n",
      " [ -1.9  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -2.8]\n",
      " [ -1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   -1.9]]\n"
     ]
    }
   ],
   "source": [
    "N_episodes = 15000\n",
    "\n",
    "def e_greedy_sample(q_of_s,epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.randint(N_actions)\n",
    "    else:\n",
    "        return np.argmax(q_of_s)\n",
    "\n",
    "#Q-Learning\n",
    "def q_learning_e_greedy(epsilon=0.1,step_size=alpha):\n",
    "    q = np.random.random(N_states * N_actions) #1\n",
    "    for terminal in terminal_states:\n",
    "        q[terminal*N_actions:(terminal+1)*N_actions] *= 0 #2\n",
    "    \n",
    "    aprox_policy = np.zeros((N_states,N_states*N_actions))\n",
    "    \n",
    "    for _ in range(N_episodes):#3\n",
    "        random_start = np.random.randint(N_states)#4\n",
    "        s = random_start\n",
    "        \n",
    "        while s not in terminal_states:#5,10\n",
    "            a = e_greedy_sample(q[s*N_actions:(s+1)*N_actions],epsilon)#6\n",
    "            r = R_(s,a)#7\n",
    "            s_t1 = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())#7\n",
    "            \n",
    "            q[s*N_actions+a] += step_size*(r+gamma*(\n",
    "                    max([q[s_t1*N_actions+a_t1] for a_t1 in range(N_actions)])\n",
    "                    - q[s*N_actions+a])) #8\n",
    "            \n",
    "            #print( '   s?',s,'a?',a,'r?',r,'v[s]?', v[s])\n",
    "            s = s_t1#9\n",
    "            \n",
    "        for s in range(N_states):#11\n",
    "            #print('state ', s)\n",
    "            arg_max = np.argmax(q[s*N_actions:(s+1)*N_actions])\n",
    "            #print('argmax ',arg_max)\n",
    "            aprox_policy[s]=np.zeros(N_states*N_actions)\n",
    "            aprox_policy[s][s*N_actions+arg_max]=1  #12\n",
    "    #print(q)        \n",
    "    return aprox_policy #13\n",
    "\n",
    "#SARSA\n",
    "def sarsa_e_greedy(epsilon=0.1,step_size=alpha):\n",
    "    q = np.random.random(N_states * N_actions) #1\n",
    "    for terminal in terminal_states:\n",
    "        q[terminal*N_actions:(terminal+1)*N_actions] *= 0 #2\n",
    "    \n",
    "    aprox_policy = np.zeros((N_states,N_states*N_actions))\n",
    "    \n",
    "    for _ in range(N_episodes):#3\n",
    "        random_start = np.random.randint(N_states)#4\n",
    "        s = random_start\n",
    "        a = e_greedy_sample(q[s*N_actions:(s+1)*N_actions],epsilon)#6\n",
    "        \n",
    "        while s not in terminal_states:#6,11\n",
    "            \n",
    "            r = R_(s,a)#7\n",
    "            s_t1 = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())#7\n",
    "            a_t1 = e_greedy_sample(q[s_t1*N_actions:(s_t1+1)*N_actions],epsilon)#8\n",
    "            q[s*N_actions+a] += step_size*(r+gamma*q[s_t1*N_actions+a_t1]\n",
    "                    - q[s*N_actions+a]) #9\n",
    "            \n",
    "            #print( '   s?',s,'a?',a,'r?',r,'v[s]?', v[s])\n",
    "            s = s_t1#10\n",
    "            a = a_t1#10\n",
    "        for s in range(N_states):#11\n",
    "            #print('state ', s)\n",
    "            arg_max = np.argmax(q[s*N_actions:(s+1)*N_actions])\n",
    "            #print('argmax ',arg_max)\n",
    "            aprox_policy[s]=np.zeros(N_states*N_actions)\n",
    "            aprox_policy[s][s*N_actions+arg_max]=1  #12\n",
    "    #print(q)        \n",
    "    return aprox_policy #13\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\nQ-LEARNING \\n')\n",
    "pi_q_learn = q_learning_e_greedy()\n",
    "env.plot_policy(pi_q_learn)\n",
    "print('\\n V with policy evaluation \\n')\n",
    "print(np.reshape(policy_evaluation_v(pi_q_learn), (4, 12))[::-1])\n",
    "\n",
    "print('\\nSARSA \\n')\n",
    "pi_sarsa = sarsa_e_greedy()\n",
    "env.plot_policy(pi_sarsa)\n",
    "print('\\n V with policy evaluation \\n')\n",
    "print(np.reshape(policy_evaluation_v(pi_sarsa), (4, 12))[::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*With enough samples it is able to learn the optimal policy*\n",
    "*With 400 timesteps none of them reached the optimal policy, with 5000 q-learning almos has it.\n",
    "With 10000 both have policies close to the optimal but not there yet. And with 15000 q- learning has the optimal policy and sarsa still has a sub-optimal policy*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
