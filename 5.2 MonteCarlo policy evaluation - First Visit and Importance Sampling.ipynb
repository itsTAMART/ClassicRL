{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.2. \n",
    "\n",
    "![img](imgs/Screenshot_2018-05-21_20-36-56.png)\n",
    "![img](imgs/Screenshot_2018-05-21_20-37-09.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as IT\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "from set_up_random_walk import Random_Walk\n",
    "\n",
    "env = Random_Walk()\n",
    "\n",
    "#initial parameters\n",
    "gamma = env.gamma\n",
    "alpha = env.alpha\n",
    "R = env.R\n",
    "P = env.P\n",
    "\n",
    "N_states = env.N_states\n",
    "N_actions = env.N_actions\n",
    "terminal_states = env.terminal_states\n",
    "\n",
    "v_ini = env.v_ini\n",
    "q_ini = env.q_ini\n",
    "\n",
    "pi_rp = env.pi_rp\n",
    "pi_opt = env.pi_opt\n",
    "pi_off = env.pi_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# function to evaluate the state value(V) function of a certain policy\n",
    "def eval_v(policy):\n",
    "    P_pi = np.matmul(policy, P)\n",
    "    R_pi = np.matmul(policy, R)\n",
    "    return np.matmul(inv(np.identity(P_pi.shape[0]) - gamma * P_pi), R_pi)\n",
    "\n",
    "# function to evaluate the state-action(Q) value function of a certain policy\n",
    "def eval_q(policy):\n",
    "    # product of the transition matrix with the policy\n",
    "    P_aux = np.matmul(P, policy)\n",
    "    return np.matmul(inv(np.identity(P_aux.shape[0]) - gamma * P_aux), R)\n",
    "\n",
    "# rewards for a state\n",
    "def R_state(state):\n",
    "    return R[N_actions * state: N_actions * state + N_actions]\n",
    "\n",
    "# transition probabilities for a state\n",
    "def P_state(state):\n",
    "    return np.flip(P[N_actions * state: N_actions * (state+ 1) ,],0)\n",
    "    #return P[N_actions * state: N_actions * (state+ 1) ,]\n",
    "\n",
    "def R_(state, action=None):\n",
    "    if action == None:\n",
    "        return R_state(state)\n",
    "    else:\n",
    "        return R_state(state)[action]\n",
    "\n",
    "def P_(state, action=None, state_t1=None):\n",
    "    if action == None:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)\n",
    "        else:\n",
    "            return P_state(state)[:, state_t1]\n",
    "    else:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)[action,]\n",
    "        else:\n",
    "            return P_state(state)[action, state_t1]\n",
    "\n",
    "def policy_(policy, state, action=None): \n",
    "    # We have to treat them different if they are lists or numpy matrices\n",
    "    if type(policy)==list:\n",
    "        if action is not None:\n",
    "            return policy[state][state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state][state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "    else:\n",
    "        if action is not None:\n",
    "            return policy[state,state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state,state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "\n",
    "def generate_episode(policy, starting_pos=None):\n",
    "    episode = []\n",
    "    #if we dont have a starting position we choose randomly from the states\n",
    "    s_t0 = np.zeros(N_states)\n",
    "    if starting_pos==None:\n",
    "        random_start = np.random.randint(N_states)\n",
    "        s_t0[random_start] = 1\n",
    "    elif isinstance(starting_pos, list): # when given an array\n",
    "        #print('array')\n",
    "        s_t0 = starting_pos\n",
    "    elif isinstance(starting_pos, int):  # when given a number\n",
    "        #print('number')\n",
    "        random_start = starting_pos\n",
    "        s_t0[random_start] = 1\n",
    "    else:                                # assume is ndarray, lazy programming, sry\n",
    "        #print('ndarray')                 # it wont work tho\n",
    "        s_t0 = starting_pos\n",
    "     \n",
    "    s = np.random.choice(N_states, p=s_t0)\n",
    "    r = None\n",
    "    while s not in terminal_states:\n",
    "        a = np.random.choice(N_actions, size=1, p=policy_(policy,s))\n",
    "        r = R_(s,a)\n",
    "        episode.append({'s':s,'a':a[0],'r':np.array(r)[0,0]})\n",
    "        \n",
    "        #next state is chosen from the transition matrix from state s taken action a\n",
    "        #s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "        s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "    episode.append({'s':s,'a':a[0],'r':0})\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Visit MC $\\pi$ evaluation of V exploring with $\\mu$ with importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_MC_pi_eval_v(policy,starting_pos=None,exploration_policy=None):\n",
    "    if not exploration_policy.any(): exploration_policy=policy\n",
    "    \n",
    "    i=0\n",
    "    v = np.zeros(N_states)#1\n",
    "    N = np.zeros(N_states)#2\n",
    "    delta = float('inf')\n",
    "    epsilon = 0.0001\n",
    "    g=0\n",
    "    while delta > epsilon:#3,13\n",
    "        delta = 0 #4\n",
    "        \"\"\"Generate an episode using pi: {s0,a0,r1,....,sT-1,aT-2,rT} \n",
    "        Each episode is a list of dictionaries containing 's_t','a_t','r_t+1'\n",
    "        \n",
    "        episode = [{'s_t':s,'a_t':a,'r_t+1':r},....]\n",
    "        T is the length of the episode\n",
    "        \"\"\"\n",
    "        episode = generate_episode(exploration_policy,starting_pos)#5\n",
    "        i+=1\n",
    "        T = len(episode)\n",
    "        #print('episode ', i,' length T: ',T)\n",
    "        g = 0 #6\n",
    "        p_arriba = 1\n",
    "        p_abajo  = 1\n",
    "        for t in reversed(range(T)): #7\n",
    "            s = episode[t]['s']\n",
    "            r = episode[t]['r']\n",
    "            a =  episode[t]['a']\n",
    "            \n",
    "            # Importance sampling\n",
    "            p_arriba *= policy_(policy,s,a)\n",
    "            p_abajo  *= policy_(exploration_policy,s,a)\n",
    "            sigma_rara = p_arriba/p_abajo\n",
    "            \n",
    "            #print(p_arriba,p_abajo)\n",
    "            g = gamma*g + r*sigma_rara #8\n",
    "            \n",
    "            #print( '   s?',s,'a?',episode[t]['a'],'r?',r,'g?', g)\n",
    "            N[s] += 1 #9\n",
    "            v_old = v[s] #10\n",
    "            v[s] = v[s] + (1/N[s])*(g - v[s]) #11\n",
    "            delta = max(delta, np.abs(v_old-v[s])) #12\n",
    "            \n",
    "        #print(delta)\n",
    "    return v  #14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It does seem to have the same problem as the other montecarlo method, if it doesn't hit a reward in the first run it doesn't converge. So if it doesn't work just run it a couple of times*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.03, 0.07, 0.11, 0.18, 0.29, 0.  ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_visit_MC_pi_eval_v(pi_off,3,pi_rp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
