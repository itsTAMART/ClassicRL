{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.1. \n",
    "\n",
    "![img](imgs/Screenshot_2018-05-21_20-36-31.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as IT\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "from set_up_random_walk import Random_Walk\n",
    "\n",
    "env = Random_Walk()\n",
    "\n",
    "#initial parameters\n",
    "gamma = env.gamma\n",
    "alpha = env.alpha\n",
    "R = env.R\n",
    "P = env.P\n",
    "\n",
    "N_states = env.N_states\n",
    "N_actions = env.N_actions\n",
    "terminal_states = env.terminal_states\n",
    "\n",
    "v_ini = env.v_ini\n",
    "q_ini = env.q_ini\n",
    "\n",
    "pi_rp = env.pi_rp\n",
    "pi_opt = env.pi_opt\n",
    "pi_off = env.pi_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# function to evaluate the state value(V) function of a certain policy\n",
    "def eval_v(policy):\n",
    "    P_pi = np.matmul(policy, P)\n",
    "    R_pi = np.matmul(policy, R)\n",
    "    return np.matmul(inv(np.identity(P_pi.shape[0]) - gamma * P_pi), R_pi)\n",
    "\n",
    "# function to evaluate the state-action(Q) value function of a certain policy\n",
    "def eval_q(policy):\n",
    "    # product of the transition matrix with the policy\n",
    "    P_aux = np.matmul(P, policy)\n",
    "    return np.matmul(inv(np.identity(P_aux.shape[0]) - gamma * P_aux), R)\n",
    "\n",
    "# rewards for a state\n",
    "def R_state(state):\n",
    "    return R[N_actions * state: N_actions * state + N_actions]\n",
    "\n",
    "# transition probabilities for a state\n",
    "def P_state(state):\n",
    "    return np.flip(P[N_actions * state: N_actions * (state+ 1) ,],0)\n",
    "    #return P[N_actions * state: N_actions * (state+ 1) ,]\n",
    "\n",
    "def R_(state, action=None):\n",
    "    if action == None:\n",
    "        return R_state(state)\n",
    "    else:\n",
    "        return R_state(state)[action]\n",
    "\n",
    "def P_(state, action=None, state_t1=None):\n",
    "    if action == None:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)\n",
    "        else:\n",
    "            return P_state(state)[:, state_t1]\n",
    "    else:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)[action,]\n",
    "        else:\n",
    "            return P_state(state)[action, state_t1]\n",
    "\n",
    "def policy_(policy, state, action=None): \n",
    "    # We have to treat them different if they are lists or numpy matrices\n",
    "    if type(policy)==list:\n",
    "        if action is not None:\n",
    "            return policy[state][state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state][state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "    else:\n",
    "        if action is not None:\n",
    "            return policy[state,state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state,state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is need for a method to create an episode using $\\pi: \\{ s_0,a_0,r_1,....,s_{T-1},a_{T-2},r_T \\} $ *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy, starting_pos=None):\n",
    "    episode = []\n",
    "    #if we dont have a starting position we choose randomly from the states\n",
    "    s_t0 = np.zeros(N_states)\n",
    "    if starting_pos==None:\n",
    "        random_start = np.random.randint(N_states)\n",
    "        s_t0[random_start] = 1\n",
    "    elif isinstance(starting_pos, list): # when given an array\n",
    "        #print('array')\n",
    "        s_t0 = starting_pos\n",
    "    elif isinstance(starting_pos, int):  # when given a number\n",
    "        #print('number')\n",
    "        random_start = starting_pos\n",
    "        s_t0[random_start] = 1\n",
    "    else:                                # assume is ndarray, lazy programming, sry\n",
    "        #print('ndarray')                 # it wont work tho\n",
    "        s_t0 = starting_pos\n",
    "     \n",
    "    s = np.random.choice(N_states, p=s_t0)\n",
    "    r = None\n",
    "    while s not in terminal_states:\n",
    "        a = np.random.choice(N_actions, size=1, p=policy_(policy,s))\n",
    "        r = R_(s,a)\n",
    "        episode.append({'s':s,'a':a[0],'r':np.array(r)[0,0]})\n",
    "        \n",
    "        #next state is chosen from the transition matrix from state s taken action a\n",
    "        #s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "        s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "    episode.append({'s':s,'a':a[0],'r':0})\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 0, 'r': 0, 's': 3},\n",
       " {'a': 0, 'r': 0, 's': 2},\n",
       " {'a': 0, 'r': 0, 's': 1},\n",
       " {'a': 0, 'r': 0, 's': 0}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the generate episode method\n",
    "generate_episode(pi_rp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'r': 0, 's': 3},\n",
       " {'a': 1, 'r': 0, 's': 4},\n",
       " {'a': 1, 'r': 1, 's': 5},\n",
       " {'a': 1, 'r': 0, 's': 6}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the generate episode method\n",
    "generate_episode(pi_opt,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Every-visit approach\n",
    "![img](imgs/5.1.1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_MC_policy_evaluation_v(policy,starting_pos=None):\n",
    "    i=0\n",
    "    v = np.zeros(N_states)#1\n",
    "    N = np.zeros(N_states)#2\n",
    "    delta = float('inf')\n",
    "    epsilon = 0.0001\n",
    "    g=0\n",
    "    while delta > epsilon:#3,13\n",
    "        delta = 0 #4\n",
    "        \"\"\"Generate an episode using pi: {s0,a0,r1,....,sT-1,aT-2,rT} \n",
    "        Each episode is a list of dictionaries containing 's_t','a_t','r_t+1'\n",
    "        \n",
    "        episode = [{'s_t':s,'a_t':a,'r_t+1':r},....]\n",
    "        T is the length of the episode\n",
    "        \"\"\"\n",
    "        episode = generate_episode(policy,starting_pos)#5\n",
    "        i+=1\n",
    "        T = len(episode)\n",
    "        #print('episode ', i,' length T: ',T)\n",
    "        g = 0 #6\n",
    "        for t in reversed(range(T)): #7\n",
    "            \n",
    "            s = episode[t]['s']\n",
    "            r = episode[t]['r']\n",
    "            g = gamma*g + r #8\n",
    "            #print( '   s?',s,'a?',episode[t]['a'],'r?',r,'g?', g)\n",
    "            N[s] += 1 #9\n",
    "            v_old = v[s] #10\n",
    "            v[s] = v[s] + (1/N[s])*(g - v[s]) #11\n",
    "            delta = max(delta, np.abs(v_old-v[s])) #12\n",
    "            \n",
    "        #print(delta)\n",
    "    return v  #14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving the Bellman equation:\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Monte-Carlo Policy evaluation every visit:\n",
      "\n",
      "[0.   0.   0.   0.81 0.9  1.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "v = eval_v(pi_opt)\n",
    "print('\\nSolving the Bellman equation:\\n')\n",
    "print(v.T)\n",
    "#If the V function is 0 repeat because it has not found any reward in the first episode and the delta still is 0\n",
    "v_MC = every_visit_MC_policy_evaluation_v(pi_opt,3)\n",
    "print('\\nMonte-Carlo Policy evaluation every visit:\\n')\n",
    "print(v_MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aqui hubo un problema porque la accion 0 debería ser izquierda y la 1 derecha, pero están funcionando al reves por lo que muchas veces se acaba el episodio sin que pase absolutamente nada y montecarlo ni huele la recompensa, por lo que no funciona porque la delta no se mueve de 0.*\n",
    "\n",
    "*Mirandolo está mal en la funcion de la matriz de transicion que devuelve las acciones cambiadas con respecto a lo que deberia ser, o de como esta en la recompensa.*\n",
    "\n",
    "*en R el 0 es izqu y el 1 derecha, pero en P estaba al reves*\n",
    "\n",
    "*Es como que algo estuviera al revés que en el resto de problemas, los demás metodos no funcionan. Hay que trucarlos cambiando la transicion de las acciones en medio de un método que estropea todos los demás.*\n",
    "\n",
    "*Respecto a MC se puede ver que si no pasa por un estado no es capaz de asignarle un valor, y en la política optima no se exploran los estados de la izaquierda.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
