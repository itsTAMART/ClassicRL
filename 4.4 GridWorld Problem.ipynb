{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Case study 4.4. The Grid-World MDP problem. Practical implementation\n",
    "\n",
    "Consider a simple grid-world problem, where the environment is a rectangular\n",
    "grid. The state of the environment is given by the location of a robot in the\n",
    "grid. Hence, the cells of the grid correspond to the possible states of the\n",
    "environment. At each cell, four actions are possible: north, south, east and\n",
    "west, which deterministically (i.e., with probability 1) cause the agent to\n",
    "move one cell in the respective direction of the grid. Actions that would\n",
    "take the robot off the grid leave its location unchanged, but also results in\n",
    "a negative (undesired) reward of ‚àí1. Other actions result in a reward of 0,\n",
    "except those that move the agent out of the special states A and B. From\n",
    "state A, all four actions yield a positive (desired) reward of +10 and take the\n",
    "agent to A 0 . From state B, all four actions yield a positive reward of +5 and\n",
    "take the agent to B 0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as IT\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "from set_up_grid_world import Grid_World\n",
    "\n",
    "env = Grid_World()\n",
    "\n",
    "#initial parameters\n",
    "gamma = env.gamma\n",
    "R = env.R\n",
    "P = env.P\n",
    "\n",
    "N_states = env.N_states\n",
    "N_actions = env.N_actions\n",
    "pi_m = env.pi_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The main problem adapting the gridworld script was not that it was long or a different language, I changed it with a couple 'find and replace' actions from a good text editor in almost 5 minutes. The problem was that every index was shifted by 1 (in matlab index start at 1 and in python and the rest of serious programming languages at 0). So the options i tried were:*\n",
    "\n",
    "1. *If the matlab script was adapted from another language script, maybe i could find the original one and shift back the indexes.*\n",
    "\n",
    "1. *More 'find and replace' and change every index to add a '-1' at the end, so they would make sense in python.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# function to evaluate the state value(V) function of a certain policy\n",
    "def eval_v(policy):\n",
    "    P_pi = np.matmul(policy, P)\n",
    "    R_pi = np.matmul(policy, R)\n",
    "\n",
    "    return np.matmul(inv(np.identity(P_pi.shape[0]) - gamma * P_pi), R_pi)\n",
    "\n",
    "\n",
    "# function to evaluate the state-action(Q) value function of a certain policy\n",
    "def eval_q(policy):\n",
    "    # product of the transition matrix with the policy\n",
    "    P_aux = np.matmul(P, policy)\n",
    "\n",
    "    return np.matmul(inv(np.identity(P_aux.shape[0]) - gamma * P_aux), R)\n",
    "\n",
    "\n",
    "# rewards for a state\n",
    "def R_state(state):\n",
    "    return R[N_actions * state: N_actions * state + N_actions]\n",
    "\n",
    "\n",
    "# transition probabilities for a state\n",
    "def P_state(state):\n",
    "    return P[N_actions * state: N_actions * state + N_actions, ]\n",
    "\n",
    "\n",
    "def R_(state, action=None):\n",
    "    if action == None:\n",
    "        return R_state(state)\n",
    "    else:\n",
    "        return R_state(state)[action]\n",
    "\n",
    "\n",
    "def P_(state, action=None, state_t1=None):\n",
    "    if action == None:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)\n",
    "        else:\n",
    "            return P_state(state)[:, state_t1]\n",
    "    else:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)[action,]\n",
    "        else:\n",
    "            return P_state(state)[action, state_t1]\n",
    "\n",
    "\n",
    "def policy_(policy, state, action=None):\n",
    "    if action is not None:\n",
    "        return policy[state][state * N_actions + action]\n",
    "    else:\n",
    "        return [policy[state][state * N_actions + action_] for action_ in range(N_actions)]\n",
    "\n",
    "\n",
    "# We define the algorithm for policy evaluation with the state value function\n",
    "def policy_evaluation_v(policy):\n",
    "    # 1\n",
    "    v = np.zeros(N_states)\n",
    "    epsilon = 0.001\n",
    "    delta = float('inf')\n",
    "    # 2, 8 stop condition is not stated, instead we do 400 iterations\n",
    "    while delta>epsilon:\n",
    "    #for i in range(N_steps):\n",
    "        # 3\n",
    "        delta = 0\n",
    "        # 4\n",
    "        for state in range(N_states):\n",
    "            # 5\n",
    "            v_old = v[state]\n",
    "            # 6\n",
    "            v_aux = 0\n",
    "            for action in range(N_actions):\n",
    "                v_aux += policy_(policy, state, action) * (R_(state, action) + gamma * sum(\n",
    "                    [P_(state, action, state_t1) * v[state_t1] for state_t1 in range(N_states)]))\n",
    "            v[state] = v_aux\n",
    "            # 7\n",
    "            delta = max(delta, np.abs(v_old - v[state]))\n",
    "\n",
    "    # 8,9\n",
    "    return v\n",
    "\n",
    "\n",
    "# We define the algorithm for policy evaluation with the state-action\n",
    "# value function\n",
    "# We define the algorithm for policy evaluation with the state value function\n",
    "def policy_evaluation_q(policy):\n",
    "    # 1\n",
    "    q = np.zeros(N_states * N_actions)\n",
    "    epsilon = 0.001\n",
    "    delta = float('inf')\n",
    "    # 2, 8 stop condition is not stated, instead we do 400 iterations\n",
    "    while delta>epsilon:\n",
    "    #for i in range(N_steps):\n",
    "        # 3\n",
    "        delta = 0\n",
    "        # 4 TO-DO\n",
    "        for state, action in IT.product(range(N_states), range(N_actions)):\n",
    "            # 5\n",
    "            q_old = q[state * N_actions + action]\n",
    "            # 6\n",
    "            q_aux = [R_(state, action)]\n",
    "            for state_t1 in range(N_states):\n",
    "                q_aux += gamma * P_(state, action, state_t1) * sum(\n",
    "                    [policy_(policy, state_t1, action_t1) * q[state_t1 * N_actions + action_t1] for action_t1 in\n",
    "                     range(N_actions)])\n",
    "            q[state * N_actions + action] = q_aux\n",
    "            # 7\n",
    "            delta = max(delta, np.abs(q_old - q[state* N_actions + action]))\n",
    "\n",
    "    return q\n",
    "\n",
    "\n",
    "# Policy iteration for state value function\n",
    "def policy_iteration_v(policy):\n",
    "    v = np.zeros(N_states)  # 1\n",
    "    theta = False\n",
    "    while not theta:  # 2,3\n",
    "        v = policy_evaluation_v(policy)  # 4-9\n",
    "        # print('\\n v:',v)\n",
    "        theta = True  # 10\n",
    "        for s in range(N_states):  # 11\n",
    "            # print('\\n s:',s)\n",
    "            a = policy_(policy, s)  # 12\n",
    "            # print('a:',a)\n",
    "\n",
    "            # 13\n",
    "            arg_max = np.argmax([(R_(s, a_t1) + gamma *\n",
    "                                  np.sum([np.dot(P_(s, a_t1, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                                 for a_t1 in range(N_actions)])\n",
    "            # print('arg_max:',arg_max)\n",
    "            policy[s] = [0] * len(policy[s])\n",
    "            policy[s][N_actions * s + arg_max] = 1\n",
    "\n",
    "            # print('pi[s]:',policy[s])\n",
    "            # print('policy_(policy, s):',policy_(policy, s))\n",
    "            if not (a == policy_(policy, s)): theta = False  # 14\n",
    "            # print('a not equal to policy_(policy, s)')\n",
    "\n",
    "            # print('theta:',theta)\n",
    "        # print('pi:',policy)\n",
    "    return policy  # 15\n",
    "\n",
    "\n",
    "# Policy iteration for state-action value function\n",
    "def policy_iteration_q(policy):\n",
    "    q = np.zeros(N_states * N_actions)  # 1\n",
    "    theta = False\n",
    "    while not theta:  # 2,3\n",
    "        q = policy_evaluation_q(policy)  # 4-9\n",
    "        # print('\\n q:',q)\n",
    "        theta = True  # 10\n",
    "        for s in range(N_states):  # 11\n",
    "            # print('\\n s:',s)\n",
    "            a = policy_(policy, s)  # 12\n",
    "            # print('a:',a)\n",
    "\n",
    "            # 13\n",
    "            arg_max = np.argmax([q[s * N_actions + a_t1] for a_t1 in range(N_actions)])\n",
    "            # print('arg_max:',arg_max)\n",
    "            policy[s] = [0] * len(policy[s])\n",
    "            policy[s][N_actions * s + arg_max] = 1\n",
    "\n",
    "            # print('pi[s]:',policy[s])\n",
    "            # print('policy_(policy, s):',policy_(policy, s))\n",
    "            if not (a == policy_(policy, s)): theta = False  # 14\n",
    "            # print('a not equal to policy_(policy, s)')\n",
    "\n",
    "            # print('theta:',theta)\n",
    "        # print('pi:',policy)\n",
    "    return policy  # 15\n",
    "\n",
    "\n",
    "def value_iteration_v():\n",
    "    policy = np.zeros([N_states, (N_states * N_actions)])\n",
    "    v = np.zeros(N_states)  # 1\n",
    "    epsilon = 0.01\n",
    "    delta = float('inf')\n",
    "    while delta > epsilon:  # 2\n",
    "        delta = 0  # 3\n",
    "\n",
    "        # print('\\n v:',v)\n",
    "        for s in range(N_states):  # 4\n",
    "            # print('\\n s:',s)\n",
    "\n",
    "            v_old = v[s]  # 5\n",
    "            # print('\\n v_old:',v_old)\n",
    "            # 6\n",
    "            v[s] = max([(R_(s, a) + gamma *\n",
    "                            np.sum([np.dot(P_(s, a, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                           for a in range(N_actions)])\n",
    "            # print('\\n v[s]:',v[s])\n",
    "            delta = max(delta, np.abs(v_old - v[s]))  # 8\n",
    "            # print('delta:',delta)\n",
    "\n",
    "    # (out of the while)\n",
    "    for s in range(N_states):  # 9\n",
    "        # 10\n",
    "        arg_max = np.argmax([(R_(s, a) + gamma *\n",
    "                              np.sum([np.dot(P_(s, a, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                             for a in range(N_actions)])\n",
    "        # print('arg_max:',arg_max)\n",
    "        policy[s] = [0] * (N_states * N_actions)\n",
    "        policy[s][N_actions * s + arg_max] = 1\n",
    "\n",
    "        # print('pi[s]:',policy[s])\n",
    "\n",
    "    # print('pi:',policy)\n",
    "    return policy  # 11\n",
    "\n",
    "\n",
    "def value_iteration_q():\n",
    "    policy = np.zeros([N_states, (N_states * N_actions)])\n",
    "    q = np.zeros(N_states * N_actions)  # 1\n",
    "    epsilon = 0.01\n",
    "    delta = float('inf')\n",
    "    while delta > epsilon:  # 2, 8\n",
    "        delta = 0  # 3\n",
    "\n",
    "        # print('\\n q:',q)\n",
    "        for s, a in IT.product(range(N_states), range(N_actions)):  # 4\n",
    "            # print('\\n s , a:',s , a)\n",
    "\n",
    "            q_old = q[s * N_actions + a]  # 5\n",
    "            # print('\\n q_old:',q_old)\n",
    "\n",
    "            # 6\n",
    "            q[s * N_actions + a] = R_(s, a) + gamma * np.sum([np.dot(P_(s, a, s_t1),\n",
    "                                                                     max([q[s_t1 * N_actions + a_t1] for a_t1 in\n",
    "                                                                             range(N_actions)]))\n",
    "                                                              for s_t1 in range(N_states)])\n",
    "\n",
    "            # print('\\n q(s,a)',q[s * N_actions + a])\n",
    "            delta = max(delta, np.abs(q_old - q[s * N_actions + a]))  # 7\n",
    "            # print('delta:',delta)\n",
    "\n",
    "    # (out of the while)\n",
    "    for s in range(N_states):  # 9\n",
    "        # 10\n",
    "        arg_max = np.argmax([q[s * N_actions + a_t1] for a_t1 in range(N_actions)])\n",
    "        # print('arg_max:',arg_max)\n",
    "        policy[s] = [0] * (N_states * N_actions)\n",
    "        policy[s][N_actions * s + arg_max] = 1\n",
    "\n",
    "        # print('pi[s]:',policy[s])\n",
    "\n",
    "    # print('pi:',policy)\n",
    "    return policy  # 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Suppose the agent selects all four actions with equal probability in all\n",
    "states (uniform random policy). Calculate analytically the value function for\n",
    "the random policy when Œ≥ = 0.9. First of all, analyze the code provided in\n",
    "grid_world_set_up.m where all involved matrices and vectors are provided as inputs to requested scripts. You should use the script ana_grid_world.m\n",
    "as the guide for all questions in this case study.\n",
    "Calculate analytically the value function for the random policy when\n",
    "Œ≥ = 0.9. For that purpose create function bellman_linear.m.\n",
    "Create function policy_evaluation.m and check that results coincide with\n",
    "those provided by bellman_linear.m and also with numbers in the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Value function calculated using the inverse: \n",
      " [[ 3.31  8.79  4.43  5.32  1.49]\n",
      " [ 1.52  2.99  2.25  1.91  0.55]\n",
      " [ 0.05  0.74  0.67  0.36 -0.4 ]\n",
      " [-0.97 -0.44 -0.35 -0.59 -1.18]\n",
      " [-1.86 -1.35 -1.23 -1.42 -1.98]]\n",
      "\n",
      " Value function calculated iteratively:       \n",
      " [[ 3.31  8.79  4.43  5.33  1.5 ]\n",
      " [ 1.53  3.    2.25  1.91  0.55]\n",
      " [ 0.05  0.74  0.68  0.36 -0.4 ]\n",
      " [-0.97 -0.43 -0.35 -0.58 -1.18]\n",
      " [-1.85 -1.34 -1.23 -1.42 -1.97]]\n",
      "\n",
      " Differences between the 2 matrices:\n",
      "[[-0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0.]]\n"
     ]
    }
   ],
   "source": [
    "# Get the value function for policy pi_m with 2 different methods\n",
    "\n",
    "v_equations = eval_v(pi_m)\n",
    "v_iteration = policy_evaluation_v(pi_m)\n",
    "\n",
    "print('\\n Value function calculated using the inverse: \\n', np.matrix(v_equations).reshape((5,5)).T)\n",
    "print('\\n Value function calculated iteratively:       \\n', np.matrix(v_iteration).reshape((5,5)).T)\n",
    "print('\\n Differences between the 2 matrices:')\n",
    "print (np.matrix(v_equations).reshape((5,5))-np.matrix(v_iteration).reshape((5,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](imgs/4.4.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b) Use your Matlab implementations of PI and VI to compute the optimal\n",
    "value function and the associated optimal policy. Implement value_iteration.m\n",
    "and policy_iteration.m to compute the optimal value function and the asso-\n",
    "ciated optimal policy. Both results should coincide and also cross-check with\n",
    "those presented in the next figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I created a fancy method for visualizing the policies with arrows (Python supports emojis üëå)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(policy):\n",
    "    symbols = ['‚¨ÜÔ∏è','‚û°Ô∏è','‚¨áÔ∏è','‚¨ÖÔ∏è']\n",
    "    plot = [''] * 25\n",
    "    for s in range(N_states):\n",
    "        plot[s] = symbols[np.argmax(policy[s,s*N_actions:s*N_actions+4])]\n",
    "        \n",
    "    print(np.reshape(plot,(5,5)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Policy calculated using the Policy Iteration: \n",
      "\n",
      "[['‚û°Ô∏è' '‚¨ÜÔ∏è' '‚¨ÖÔ∏è' '‚¨ÜÔ∏è' '‚¨ÖÔ∏è']\n",
      " ['‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÖÔ∏è' '‚¨ÖÔ∏è']\n",
      " ['‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è']\n",
      " ['‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è']\n",
      " ['‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è']]\n",
      "\n",
      " Policy calculated using the Value Iteration:  \n",
      "\n",
      "[['‚û°Ô∏è' '‚¨ÜÔ∏è' '‚¨ÖÔ∏è' '‚¨ÜÔ∏è' '‚¨ÖÔ∏è']\n",
      " ['‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÖÔ∏è' '‚¨ÖÔ∏è']\n",
      " ['‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è']\n",
      " ['‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è']\n",
      " ['‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è' '‚¨ÜÔ∏è']]\n",
      "\n",
      " MSE Differences between the 2 policies: 0.0\n",
      "\n",
      " \n",
      " Value function of the optimal policy: \n",
      " [[21.98 24.42 21.98 19.42 17.48]\n",
      " [19.78 21.98 19.78 17.8  16.02]\n",
      " [17.8  19.78 17.8  16.02 14.42]\n",
      " [16.02 17.8  16.02 14.42 12.98]\n",
      " [14.42 16.02 14.42 12.98 11.68]]\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal policy with 2 different methods\n",
    "\n",
    "policy_PI = policy_iteration_q(pi_m)\n",
    "policy_VI = value_iteration_q()\n",
    "\n",
    "print('\\n Policy calculated using the Policy Iteration: \\n')\n",
    "plot_policy(policy_PI)\n",
    "print('\\n Policy calculated using the Value Iteration:  \\n')\n",
    "plot_policy(policy_PI)\n",
    "print('\\n MSE Differences between the 2 policies:', np.sum(np.square(np.matrix(policy_PI)-np.matrix(policy_PI))))\n",
    "\n",
    "\n",
    "v_equations = eval_v(policy_PI)\n",
    "\n",
    "print('\\n \\n Value function of the optimal policy: \\n', np.matrix(v_equations).reshape((5,5)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](imgs/4.4.2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
