{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.4. \n",
    "\n",
    "![img](imgs/5.4.1.png)\n",
    "![img](imgs/5.4.2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as IT\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "from set_up_random_walk import Random_Walk\n",
    "\n",
    "env = Random_Walk()\n",
    "\n",
    "#initial parameters\n",
    "gamma = env.gamma\n",
    "alpha = env.alpha\n",
    "R = env.R\n",
    "P = env.P\n",
    "\n",
    "N_states = env.N_states\n",
    "N_actions = env.N_actions\n",
    "terminal_states = env.terminal_states\n",
    "\n",
    "v_ini = env.v_ini\n",
    "q_ini = env.q_ini\n",
    "\n",
    "pi_rp = env.pi_rp\n",
    "pi_opt = env.pi_opt\n",
    "pi_off = env.pi_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# function to evaluate the state value(V) function of a certain policy\n",
    "def eval_v(policy):\n",
    "    P_pi = np.matmul(policy, P)\n",
    "    R_pi = np.matmul(policy, R)\n",
    "    return np.matmul(inv(np.identity(P_pi.shape[0]) - gamma * P_pi), R_pi)\n",
    "\n",
    "# function to evaluate the state-action(Q) value function of a certain policy\n",
    "def eval_q(policy):\n",
    "    # product of the transition matrix with the policy\n",
    "    P_aux = np.matmul(P, policy)\n",
    "    return np.matmul(inv(np.identity(P_aux.shape[0]) - gamma * P_aux), R)\n",
    "\n",
    "# rewards for a state\n",
    "def R_state(state):\n",
    "    return R[N_actions * state: N_actions * state + N_actions]\n",
    "\n",
    "# transition probabilities for a state\n",
    "def P_state(state):\n",
    "    return np.flip(P[N_actions * state: N_actions * (state+ 1) ,],0)\n",
    "    #return P[N_actions * state: N_actions * (state+ 1) ,]\n",
    "\n",
    "def R_(state, action=None):\n",
    "    if action == None:\n",
    "        return R_state(state)\n",
    "    else:\n",
    "        return R_state(state)[action]\n",
    "\n",
    "def P_(state, action=None, state_t1=None):\n",
    "    if action == None:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)\n",
    "        else:\n",
    "            return P_state(state)[:, state_t1]\n",
    "    else:\n",
    "        if state_t1 == None:\n",
    "            return P_state(state)[action,]\n",
    "        else:\n",
    "            return P_state(state)[action, state_t1]\n",
    "\n",
    "def policy_(policy, state, action=None): \n",
    "    # We have to treat them different if they are lists or numpy matrices\n",
    "    if type(policy)==list:\n",
    "        if action is not None:\n",
    "            return policy[state][state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state][state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "    else:\n",
    "        if action is not None:\n",
    "            return policy[state,state * N_actions + action]\n",
    "        else:\n",
    "            return [policy[state,state * N_actions + action_] for action_ in reversed(range(N_actions))]\n",
    "\n",
    "def generate_episode(policy, starting_pos=None):\n",
    "    episode = []\n",
    "    #if we dont have a starting position we choose randomly from the states\n",
    "    s_t0 = np.zeros(N_states)\n",
    "    if starting_pos==None:\n",
    "        random_start = np.random.randint(N_states)\n",
    "        s_t0[random_start] = 1\n",
    "    elif isinstance(starting_pos, list): # when given an array\n",
    "        #print('array')\n",
    "        s_t0 = starting_pos\n",
    "    elif isinstance(starting_pos, int):  # when given a number\n",
    "        #print('number')\n",
    "        random_start = starting_pos\n",
    "        s_t0[random_start] = 1\n",
    "    else:                                # assume is ndarray, lazy programming, sry\n",
    "        #print('ndarray')                 # it wont work tho\n",
    "        s_t0 = starting_pos\n",
    "     \n",
    "    s = np.random.choice(N_states, p=s_t0)\n",
    "    r = None\n",
    "    while s not in terminal_states:\n",
    "        a = np.random.choice(N_actions, size=1, p=policy_(policy,s))\n",
    "        r = R_(s,a)\n",
    "        episode.append({'s':s,'a':a[0],'r':np.array(r)[0,0]})\n",
    "        \n",
    "        #next state is chosen from the transition matrix from state s taken action a\n",
    "        #s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "        s = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())\n",
    "    episode.append({'s':s,'a':0,'r':0})\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.  ,  0.07,  0.15,  0.26,  0.43,  0.69,  0.  ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(eval_v(pi_rp).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fsolve is broken ¯\\_( ツ )_/¯ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_steps = 400\n",
    "# We define the algorithm for policy evaluation with the state value function\n",
    "def policy_evaluation_v(policy):\n",
    "    # 1 \n",
    "    v = np.zeros(N_states)\n",
    "    # 2, 8 stop condition is not stated, instead we do 400 iterations\n",
    "    for i in range(N_steps):\n",
    "        # 3\n",
    "        delta = 0\n",
    "        # 4\n",
    "        for state in range(N_states):\n",
    "            # 5 \n",
    "            v_old = v[state]\n",
    "            # 6 \n",
    "            v_aux = 0\n",
    "            for action in range(N_actions):\n",
    "                v_aux += policy_(policy, state, action) * (R_(state, action) + gamma * sum(\n",
    "                    [P_(state, action, state_t1) * v[state_t1] for state_t1 in range(N_states)]))\n",
    "            v[state] = v_aux\n",
    "            # 7\n",
    "            # delta = np.max(delta, np.abs(v_old - v[state]))\n",
    "\n",
    "    # 8,9    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.07, 0.15, 0.26, 0.43, 0.69, 0.  ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_evaluation_v(pi_rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy iteration for state value function\n",
    "def policy_iteration_v(pi_input, debug=False):\n",
    "    policy = np.copy(pi_input)\n",
    "    v = np.zeros(N_states) #1\n",
    "    theta = False\n",
    "    while not theta: #2,3\n",
    "        v = policy_evaluation_v(policy) # 4-9\n",
    "        if debug : print('\\n v:',v)\n",
    "        theta = True #10\n",
    "        if debug : print('For each state: ')\n",
    "        for s in range(N_states): #11\n",
    "            if debug : print('\\n s:',s)\n",
    "            a = policy_(policy, s) #12\n",
    "            if debug : print('a:',a)  \n",
    "            \n",
    "            #13\n",
    "            arg_max = np.argmax([(R_(s, a_t1) + gamma *\n",
    "                                 np.sum([np.dot(P_(s, a_t1, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                                 for a_t1 in range(N_actions)])\n",
    "            if debug : print('arg_max:',arg_max)\n",
    "            policy[s] = [0] * len(policy[s])\n",
    "            policy[s,N_actions * s + arg_max] = 1\n",
    "            \n",
    "            if debug : print('pi[s]:',policy[s])\n",
    "            if debug : print('policy_(policy, s):',policy_(policy, s))\n",
    "            if not (a == policy_(policy, s)): \n",
    "                theta = False #14\n",
    "                if debug : print('a not equal to policy_(policy, s)')\n",
    "                \n",
    "            if debug : print('theta:',theta)\n",
    "        if debug : print('\\n pi:',policy)\n",
    "    return policy #15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['⬅️', '➡️', '➡️', '➡️', '➡️', '➡️', '⬅️']\n"
     ]
    }
   ],
   "source": [
    "env.plot_policy(policy_iteration_v(pi_rp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_v(debug=False):\n",
    "    if debug : print('Value Iteration with V:')\n",
    "    policy = np.zeros([N_states,(N_states*N_actions)]) \n",
    "    v = np.zeros(N_states) #1\n",
    "    epsilon = 0.01\n",
    "    delta = 10000000\n",
    "    while delta>epsilon: #2\n",
    "        delta = 0 #3\n",
    "        \n",
    "        if debug : print('\\n v:',v)\n",
    "        for s in range(N_states): #4\n",
    "            if debug : print('\\n s:',s)\n",
    "            \n",
    "            v_old = v[s] #5\n",
    "            if debug : print('\\n v_old:',v_old)\n",
    "            #6\n",
    "            v[s]=np.max([(R_(s, a) + gamma *\n",
    "                         np.sum([np.dot(P_(s, a, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                         for a in range(N_actions)])\n",
    "            if debug : print('\\n v[s]:',v[s])\n",
    "            delta = max(delta, np.abs(v_old-v[s]))#8\n",
    "            if debug : print('delta:',delta)\n",
    "\n",
    "    # (out of the while) \n",
    "    if debug : print('\\n For each state: \\n')\n",
    "    for s in range(N_states): #9\n",
    "        #10\n",
    "        arg_max = np.argmax([(R_(s, a) + gamma *\n",
    "                             np.sum([np.dot(P_(s, a, s_t1), v[s_t1]) for s_t1 in range(N_states)]))\n",
    "                             for a in range(N_actions)])\n",
    "        if debug : print('arg_max:',arg_max)\n",
    "        policy[s] = [0] * (N_states*N_actions)\n",
    "        policy[s,N_actions * s + arg_max] = 1\n",
    "            \n",
    "        if debug : print('pi[s]:',policy[s])\n",
    "\n",
    "\n",
    "    if debug : print('pi:',policy)\n",
    "    return policy #11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['⬅️', '➡️', '➡️', '➡️', '➡️', '➡️', '⬅️']\n"
     ]
    }
   ],
   "source": [
    "env.plot_policy(value_iteration_v())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_episodes = 800\n",
    "\n",
    "def temporal_difference_v(policy):\n",
    "    v = np.zeros(N_states)#1\n",
    "    for _ in range(N_episodes):#2,10\n",
    "        random_start = np.random.randint(N_states)#3\n",
    "        s = random_start\n",
    "        while s not in terminal_states:#4,9\n",
    "            a = np.random.choice(N_actions, size=1, p=policy_(policy,s))#5\n",
    "            r = R_(s,a)#6\n",
    "            s_t1 = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())#6\n",
    "            v[s] = v[s] + alpha*(r+v[s_t1]-v[s])#7\n",
    "            #print( '   s?',s,'a?',a,'r?',r,'v[s]?', v[s])\n",
    "            s = s_t1#8\n",
    "    return v #11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving the value function with TD methods:\n",
      "\n",
      "[0.   0.15 0.26 0.42 0.65 0.83 0.  ]\n"
     ]
    }
   ],
   "source": [
    "v = temporal_difference_v(pi_rp)\n",
    "print('\\nSolving the value function with TD methods:\\n')\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.\n",
    "\n",
    "![img](imgs/5.4.4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_episodes = 1400\n",
    "\n",
    "def e_greedy_sample(q_of_s,epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.randint(N_actions)\n",
    "    else:\n",
    "        return np.argmax(q_of_s)\n",
    "\n",
    "\n",
    "def q_learning_e_greedy(epsilon=0.1,step_size=alpha):\n",
    "    q = np.random.random(N_states * N_actions) #1\n",
    "    for terminal in terminal_states:\n",
    "        q[terminal*N_actions:(terminal+1)*N_actions] *= 0 #2\n",
    "    \n",
    "    aprox_policy = np.zeros((N_states,N_states*N_actions))\n",
    "    \n",
    "    for _ in range(N_episodes):#3\n",
    "        random_start = np.random.randint(N_states)#4\n",
    "        s = random_start\n",
    "        \n",
    "        while s not in terminal_states:#5,10\n",
    "            a = e_greedy_sample(q[s*N_actions:(s+1)*N_actions],epsilon)#6\n",
    "            r = R_(s,a)#7\n",
    "            s_t1 = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())#7\n",
    "            \n",
    "            q[s*N_actions+a] += step_size*(r+gamma*(\n",
    "                    max([q[s_t1*N_actions+a_t1] for a_t1 in range(N_actions)])\n",
    "                    - q[s*N_actions+a])) #8\n",
    "            \n",
    "            #print( '   s?',s,'a?',a,'r?',r,'v[s]?', v[s])\n",
    "            s = s_t1#9\n",
    "            \n",
    "        for s in range(N_states):#11\n",
    "            #print('state ', s)\n",
    "            arg_max = np.argmax(q[s*N_actions:(s+1)*N_actions])\n",
    "            #print('argmax ',arg_max)\n",
    "            aprox_policy[s]=np.zeros(N_states*N_actions)\n",
    "            aprox_policy[s][s*N_actions+arg_max]=1  #12\n",
    "    print(q)        \n",
    "    return aprox_policy #13\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.17 1.05 0.68 1.11 0.87 1.11 1.   1.11 0.64 1.11 0.   0.  ]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "['⬅️', '➡️', '➡️', '➡️', '➡️', '➡️', '⬅️']\n"
     ]
    }
   ],
   "source": [
    "pi_q_learn = q_learning_e_greedy()\n",
    "print(pi_q_learn)\n",
    "env.plot_policy(pi_q_learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here if the iterations are too low it doesn't converge to the optimal policy,with experiments with 800 episodes it didn't converge always. But with 1400 it works perfectly fine*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.\n",
    "\n",
    "![img](imgs/5.4.3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_e_greedy(epsilon=0.1,step_size=alpha):\n",
    "    q = np.random.random(N_states * N_actions) #1\n",
    "    for terminal in terminal_states:\n",
    "        q[terminal*N_actions:(terminal+1)*N_actions] *= 0 #2\n",
    "    \n",
    "    aprox_policy = np.zeros((N_states,N_states*N_actions))\n",
    "    \n",
    "    for _ in range(N_episodes):#3\n",
    "        random_start = np.random.randint(N_states)#4\n",
    "        s = random_start\n",
    "        a = e_greedy_sample(q[s*N_actions:(s+1)*N_actions],epsilon)#6\n",
    "        \n",
    "        while s not in terminal_states:#6,11\n",
    "            \n",
    "            r = R_(s,a)#7\n",
    "            s_t1 = np.random.choice(N_states, p=np.asarray(P_(s,a)).ravel())#7\n",
    "            a_t1 = e_greedy_sample(q[s_t1*N_actions:(s_t1+1)*N_actions],epsilon)#8\n",
    "            q[s*N_actions+a] += step_size*(r+gamma*q[s_t1*N_actions+a_t1]\n",
    "                    - q[s*N_actions+a]) #9\n",
    "            \n",
    "            #print( '   s?',s,'a?',a,'r?',r,'v[s]?', v[s])\n",
    "            s = s_t1#10\n",
    "            a = a_t1#10\n",
    "        for s in range(N_states):#11\n",
    "            #print('state ', s)\n",
    "            arg_max = np.argmax(q[s*N_actions:(s+1)*N_actions])\n",
    "            #print('argmax ',arg_max)\n",
    "            aprox_policy[s]=np.zeros(N_states*N_actions)\n",
    "            aprox_policy[s][s*N_actions+arg_max]=1  #12\n",
    "    print(q)        \n",
    "    return aprox_policy #13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.23 0.56 0.29 0.66 0.4  0.76 0.49 0.87 0.53 1.   0.   0.  ]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "['⬅️', '➡️', '➡️', '➡️', '➡️', '➡️', '⬅️']\n"
     ]
    }
   ],
   "source": [
    "pi_sarsa = sarsa_e_greedy()\n",
    "print(pi_sarsa)\n",
    "env.plot_policy(pi_sarsa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
