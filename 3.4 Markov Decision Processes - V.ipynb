{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.4. \n",
    "Consider the state diagram shown in Figure (3.8) describing\n",
    "a *Random Walk* problem. All episodes start in the center state, C, and\n",
    "proceed either left or right by one state on each step, with equal probability.\n",
    "Episodes terminate either on the extreme left or the extreme right. When\n",
    "an episode terminates on the right, a reward of +1 occurs; all other rewards\n",
    "are zero. For example, a typical episode might consist of the following state-\n",
    "and-reward sequence: C, 0, B, 0, C, 0, D, 0, E, 1. Consider Î³ = 0.9. The\n",
    "goal is to compute the state value function for every state applying the fixed\n",
    "point solution to the corresponding Bellman equation.\n",
    "![img](imgs/Screenshot_2018-04-20_12-29-13.png)\n",
    "![img](imgs/Screenshot_2018-04-20_12-30-39.png)\n",
    "The policy matrix:\n",
    "![img](imgs/Screenshot_2018-04-20_12-32-22.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "#initial parameters\n",
    "\n",
    "gamma = .9\n",
    "\n",
    "R = np.matrix([[0,0,0,0,0,0,0,0,0,0,0,1,0,0]]).T\n",
    "\n",
    "P = np.matrix([[1,0,0,0,0,0,0],\n",
    "[1,0,0,0,0,0,0],\n",
    "[0,0,1,0,0,0,0],\n",
    "[1,0,0,0,0,0,0],\n",
    "[0,0,0,1,0,0,0],\n",
    "[0,1,0,0,0,0,0],\n",
    "[0,0,0,0,1,0,0],\n",
    "[0,0,1,0,0,0,0],\n",
    "[0,0,0,0,0,1,0],\n",
    "[0,0,0,1,0,0,0],\n",
    "[0,0,0,0,0,0,1],\n",
    "[0,0,0,0,1,0,0],\n",
    "[0,0,0,0,0,0,1],\n",
    "[0,0,0,0,0,0,1]])\n",
    "\n",
    "pi_m = np.matrix([[.5,.5,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "[0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0],\n",
    "[0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0],\n",
    "[0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0],\n",
    "[0,0,0,0,0,0,0,0,0.5,0.5,0,0,0,0],\n",
    "[0,0,0,0,0,0,0,0,0,0,0.5,0.5,0,0],\n",
    "[0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **exercise 3.3** we reuse the function to evaluate a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "#function to evaluate the state value(V) function of a certain policy\n",
    "def eval_v(policy):\n",
    "    P_pi = np.matmul(policy,P)\n",
    "    R_pi = np.matmul(policy,R)\n",
    "    \n",
    "    return np.matmul(inv(np.identity(P_pi.shape[0]) - gamma * P_pi) , R_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.  ]\n",
      " [ 0.07]\n",
      " [ 0.15]\n",
      " [ 0.26]\n",
      " [ 0.43]\n",
      " [ 0.69]\n",
      " [ 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "v = eval_v(pi_m)\n",
    "print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
